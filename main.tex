\documentclass[screen,acmsmall]{acmart}
\settopmatter{printfolios=true,printccs=true,printacmref=true}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}   %% For author/year citations
\usepackage{mathpartir}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{microtype}
\usepackage{listings,array,multirow,wrapfig,xspace,booktabs,subcaption}
\usepackage{xcolor,tikz, graphicx}
\usetikzlibrary{positioning}

\newcommand{\authorcomment}[3]{\xspace\textcolor{#1}{{\bf #2} #3}\xspace} % start by defining an authorcomment

% For author notes:
%
\newcommand{\AG}[1]{\authorcomment{orange}{AG}{#1}}
\newcommand{\JV}[1]{\authorcomment{red}{JV}{#1}}

% For meta comments:
%
\newcommand{\isit}[1]{\authorcomment{cyan}{Check}{#1}}
\newcommand{\todo}[1]{\authorcomment{red}{TODO}{#1}}


\lstset{language=R}

\definecolor{LightGray}{rgb}{.92,.92,.92}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

\lstset{ %
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=-1mm,
  framexrightmargin=-1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray}
}
\lstdefinestyle{R}{ %
  language=R,
  deletekeywords={new, env, equal, c, runif, trace, args, exp, t, all, get,
    names, is, environment, class, substitute, expression, list, null, Internal,
    sample, diag, length, rep, nrow, stop, offset, pmax, Machine,
    double, parent, frame, par, methods, end, dir, apply, deparse, missing,
    plot, as, integer, character, inherits, numeric, paste, eval, quote, call,
    formula, df, log, sum, c, local, legend, file, scale, round, title, order,
    drop, which, grid, print, ncol, dim, max, format, sort, rug, matrix, start,
    unique, mean, df, attr, do, power},
  otherkeywords={},
  breaklines=true
}

\newcommand{\code}[1]{\lstinline[style=R]|#1|\xspace}
\renewcommand{\c}[1]{\lstinline[style=R]|#1|\xspace}

%%% \setcopyright{rightsretained}
%%% \acmPrice{}
%%% \acmDOI{10.1145/3360579}
%%% \acmYear{2019}
%%% \copyrightyear{2019}
%%% \acmJournal{PACMPL}
%%% \acmVolume{3}
%%% \acmNumber{OOPSLA}
%%% \acmArticle{153}
%%% \acmMonth{10}
\begin{document}
\title{Promises are made to be broken}
\subtitle{On providing strict evaluation semantics for the R language}

\author{Aviral Goel}\affiliation{\institution{Northeastern University}\country{USA}}
\author{Jan Ječmen}\affiliation{\institution{Czech Technical University}\country{Czechia}}
\author{Sebastián Krynski}\affiliation{\institution{Czech Technical University}\country{Czechia}}
\author{Oliver Flückiger}\affiliation{\institution{Northeastern University}\country{USA}}
\author{Jan Vitek}\affiliation{\institution{Czech Technical University and Northeastern University}\country{USA}}
\authorsaddresses{}
\renewcommand{\shortauthors}{Goel, Vitek}

\begin{abstract}
  Function calls in the R language do not evaluate their arguments, these are
  passed as suspended computations to the callee which will evaluate them only
  if they are needed. After 25 years of experience with the language, there are
  very few cases where delayed evaluation is being intentionally leveraged by
  programmers. Yet being lazy comes at a price in performance and complexity.
  This paper explores what would happen if the semantics of the language was
  changed to become strict-by-default and lazy-on-demand. To answer this
  question we implemented a dynamic analysis that synthesizes strictness
  signatures for functions. Given such signature, we implemented a tool that
  automatically transforms source code to enforce the signatures. We then
  performed a large scale evaluation of the robustness of the inferred
  signature. Finally we explored the impact of providing strictness signatures
  on a just-in-time compiler.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002944.10011123.10010912</concept_id>
<concept_desc>General and reference~Empirical studies</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10010517</concept_id>
<concept_desc>Software and its engineering~Scripting languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011039.10011311</concept_id>
<concept_desc>Software and its engineering~Semantics</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{General and reference~Empirical studies}
\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[500]{Software and its engineering~Scripting languages}
\ccsdesc[300]{Software and its engineering~Semantics}

\keywords{R language, delayed or lazy evaluation}

\maketitle
\section{Introduction}

The R programming language is widely used in data science. Unbeknownst to many
of its end-users, function calls have a call-by-need, or lazy, semantics. In
other words the arguments of a function call are suspended computations which
are evaluated if and when they are needed. \citet{oopsla19b} provided a thorough
observational study of a corpus consisting of 16,707 packages written in R. For
the most part, the corpus appears to have been written without reliance on
laziness with the exception of code that leverages it for meta-programming.

This paper argues that laziness should be the exception in R. We propose to make
it a \emph{eager by default, lazy on demand} language by introducing strictness
annotations. The question we wish to answer is whether it is possible to switch
the semantics of a language without causing undue breakage in the legacy code
that is in daily use. This concern is relevant because, even if programmers
don't avail themselves of call-by-need, it is conceivable that their code
accidentally depends on it. A change in the order of evaluation of arguments may
introduce errors if that code was performing side effects.

\paragraph{The Case for Strictness.} Laziness is error-prone, inconsistent
and costly. The combination of delayed evaluation and side-effects in a language
without type annotations is an invitation to subtle programming errors. If a
function has multiple evaluation orders and is called with effectful arguments,
their effects will be observed to happen in various orders. Functional languages
prevent ordering issues by reflecting effects in the type system. R cannot do
this as it is dynamically typed. Instead, libraries routinely add code at the
boundaries of their API to force a single evaluation order. The design and use
of call-by-need is inconsistent because there are multiple points where
evaluation is arbitrarily forced. These include right-hand side of assignments
and function returns, neither of which is required in a lazy language.
Furthermore, to support object-oriented programming, R allows both single- and
multiple-dispatch; in order to perform dispatch, arguments must be evaluated
eagerly. Last, there are costs to lazy evaluation. Each argument must be boxed
in a data structure that holds a reference to the expression to evaluate, its
environment and the result of evaluation. Allocating and deallocating these data
structures put pressure on the memory manager. Delayed evaluation further
complicates the work of compilers for the language by hindering optimizations
and increasing the number of indirect calls.

\paragraph{The Case for Laziness.} The success of a programming language
often comes down to the strength of its ecosystem. With tens of millions of
lines of contributed library code, any change to the core semantics of the
language risks changing the behavior of some library functions. Preserving the
status quo is a pragmatic choice to protect the investment in legacy code bases.
Laziness is needed for an additional reason, it is the building block of the
meta-programming facilities of the language. Unevaluated arguments can be
coerced back to their source code, that code can be modified, and evaluated in
an environment of the programmer's choice. Meta-programming is used to extend
the language and to create embedded domain specific languages. While it is
possible to imagine using macros instead, the number of libraries that use
meta-programming is sufficiently large that it would be non-trivial to refactor
them.

\section{Background}
\subsection{The R Language}

\subsection{Related Work}
The foundation for our work is \cite{oopsla19b}'s work on the design,
implementation, and use of laziness in R. They provide a detailed account of R's
call-by-need evaluation strategy with a small-step operational semantics and an
empirical evaluation of 16,707 R packages. Their study shows that R is rather
strict for a lazy language, with most R code written without reliance on
laziness. In their corpus of 388.3K functions, they identify 83.7\% functions as
strict, i.e., these functions evaluate all of their arguments in a single order
across all calls. The authors find that R programmers ``force'' arguments at the
beginning of a function for a deterministic evaluation order. The authors
identify only one instance of lazy data structure and find that meta-programming
is the only dominant use of R's laziness.

\cite{oopsla20b}'s work on empirically inferring type signatures for R functions
is very similar to our work in spirit. They synthesize function type signatures
by dynamically observing the type of function arguments and return values. They
validate these signatures by inserting type checking code inside functions and
monitor type contract failure by running client programs. Failed type contracts
can be detected at a function's boundary by comparing a value's actual type
against its expected type. However, evaluating an argument on function entry
does not prohibit it from being used in a non-strict setting such as
metaprogramming. A side-effecting argument which would otherwise not have been
evaluated by the function could still be evaluated according to our strictness
signature without an observable error. This makes validation of strictness
signatures hard.

With this work, we are trying to propose a change in the semantics of R. There
are many other instances of mainstream languages incorporating a major change in
semantics. The most prominent example is the transition from Python 2 to Python
3, which is still an ongoing process. Python 3 was released in 2008 with a
plan\cite{pysunset} to drop support for Python 2 in 2015. This was later
extended to 2020 since a lot of codebase in 2015 was not transitioned to Python
3. However, legcay codebase continues to run on Python 2 till date. Python 3
introduced many incompatible changes without any tooling support for transition.
py2to3 script only handles a selection of superficial changes. TODO.

TODO: Autobahn (haskell strictness signature).

Similar to our attempts to modify R's semantics are the transition from Python 2
to Python 3, PHP to Hack, introduction of Javascript's strict mode and 

\section{Strictness Signatures}

A function's strictness signature is a tuple $\langle p, f, s_1, ..., s_n
\rangle$, where $f$ is the function's qualified name and $s_i$ are positions of
arguments that can be evaluted strictly. We synthesize these strictness
signatures for functions from dynamically generated traces. The choice of making
a function strict in a particular argument is condititioned on many aspects of
its evaluation, not just on that fact that it is evaluated. The combination of
these choices result in many possible signature configurations.
argument can be evaluated eagerly is The ``features'' supported by R introduce
many challenges

\subsection{vararg}
varargs, or $...$ paramaters, accept an arbitrary number of arguments, including
missing arguments. The function can materialize $...$ into a list using the
\code{list(...)} pattern. It can also forward $...$ to a callee. This lets the
function expose its callee's interface to the callers. A combination of $...$
and other arguments passed by a caller can match against a callee's $...$ and
this could go on transitively until the $...$ arguments are unpacked against the
parameter list of the final function in the chain. Assigning a single strictness
annotation to $...$ is tricky because a function can have different
strictness behavior for the individual arguments. Assigning a unique strictness
signature to $...$ is tricky because the outer function may use them in a strict
setting but the callees may metaprogram on the promises packaged inside $...$.
The presence of missing arguments inside $...$ introduces another challenge.
We treat $...$ parameters as lazy in our signatures.


 


for
\emph{(a)} to forward the arguments to a callee \emph{(b)} to number of
arguments for strictness signatures. strict. ... effect as.environment

Firstly, 
The first source of laziness is when an argument is not
used in a call to the function. Such arguments could still be evaluated strictly
but there might be a significant performance penalty, or worse, a side-effect.
Even reads performed by the argument could trigger evaluation of promises that
could lead to side-effects.

There are 2 internal sources of
laziness in R: metaprogramming, and unused arguments.


\emph{Metaprogramming}

\emph{Effects}

\emph{Reflection}

\subsection{Synthesizing Signatures}

\section{Analysis Infrastructure}
In this section, we describe the analysis pipeline that \emph{(a)} profiles R
programs for laziness, \emph{(b)} extracts and analyzes strictness signatures
from the profile, and \emph{(c)} validates those signatures by eagerly forcing
function arguments. The analysis pipeline starts with setting up a Docker image
that includes all the dependencies for installing analysis code and R packages
from CRAN and Bioconductor. This provides a reliable reproducible setup across
the three machines. Next, we mirror CRAN and Bioconductor\cite{bioc} repositories and
install their R packages. This is followed by generation of execution traces
from a dynamic analyzer running inside an instrumented R virtual machine. These
traces are analyzed to generate tabular data files and strictness signatures.
Finally, the signatures are applied to client programs for validation. Our
experiments were performed on three Intel Xeon 6140, 2.30GHz machines with 72
cores and 256GB of RAM each. The pipeline is managed by a Makefile which has
rules for every step. This makes it easy to administer the experiments on
multiple machines. Whenever possible, we parallelize the steps using GNU
parallel\cite{tange2011a}.

\subsection{Synthesizing Strictness Signatures}

A function's strictness signature is a tuple $\langle f, s_1, ..., s_n
\rangle$, where $f$ is the function's qualified name and $s_i$ are positions of arguments that can be
evaluted strictly. We synthesize these strictness signatures for functions by
observing the evaluation of their arguments at runtime. For this, we use three
tools that enable us to dynamically inspect a function call's arguments.

%% R-dyntrace

First,
we use an instrumented R virtual machine, \emph{R-dyntrace} \citet{oopsla19b}
based on GNU-R version 4.0.2. The framework provides an event framework for
invoking user-defined callbacks at specific points inside the R interpreter.
\emph{R-dyntrace} supplies raw R objects to the callbacks. Extracting meaningful
information at this low level requires handling a lot of subtleties arising from
the intricacies of R's implementation and execution model. These details are
surprisingly hard to get right, even for simple progrms. The challenges are
described in detail by \cite{oopsla19b} in the design of their tracer for
collecting information about use of laziness in R programs.

%% instrumentr

To iron these wrinkles, we developed \emph{instrumentr}, an R package that
provides a layer of abstraction on top of the event framework exposed by
\emph{R-dyntrace}. \emph{instrumentr} provides API to create tracer objects to
which event specific callbacks can be attached. \emph{instrumentr} intercepts
\emph{R-dyntrace} events and invokes the corresponding tracer's callbacks. It
passes model objects to callbacks by wrapping the raw R objects provided to
its own callbacks registered with \emph{R-dyntrace}.
\begin{itemize}
\item These model objects contain metadata about the raw R object and provide a
  consistent API for inspecting their constituents.
\item The object metadata keeps track of the object's unique id. This uniqueness
  guarantee enables downstream client analyses to key objects by their id
  instead of addresses which are not unique since they get resused by R's
  garbage collector.
\item The model objects are reference counted which makes it possible for a
  model object to be referenced by multiple model objects with proper memory
  management. A complication arises in this design when objects have cyclic
  references. For example, a promise argument keeps a reference to a call object
  and vice-versa. To address this, \emph{instrumentr} defines the notion of a
  primary owner. The primary owner of a model object can ``kill'' the object,
  i.e. free up object's internals (and decrement the references it holds to
  other objects) even if it is referenced by other objects. Killing does not
  free up the model object's memory or affect its metadata. This lets secondary
  owners access the dead object's metadata even after the raw R object no longer
  exists. The ``dead'' object's memory is finally freed when it is no longer
  referenced by any other object in the system. This design insight makes it
  trivial to express complex object dependencies without any memory leaks at the
  scale of millions of objects. This also enables clients to query dead object's
  metadata if they are referenced by model objects made available to the clients
  via their callbacks. This greatly simplifies the handling of promises, first
  class environments and function calls.
\item \emph{instrumentr} keeps track of logical time, from tracing entry to
tracing exit, incremented on every event. The model object's metadata keeps
track of its time of allocation and deallocation. For environments, it also
keeps track of last read and last write time. For promises, it keeps track of
force entry and force exit time. This information is used to identify non-local
reads and writes to environments. The model objects are cached in a table keyed
by the R object's address. Table entries are inserted and erased on \emph{object
  allocation and deallocation} events respectively. This prevents duplication of
model objects when the same R object is encountered on multiple events.
\item \emph{instrumentr} models the call stack. It also adds promise objects under
evaluation to the stack. This helps identify if an event, say a side-effect, is
occuring inside a promise. Furthermore, R uses \emph{longjmp} to do non-local
returns. \emph{instrumentr} exposes a deterministic behavior during such events
by artificially calling the exit callbacks of interrupted calls and promises on
the stack. This simplifies the client side tracing logic, significantly.
\item \emph{instrumentr} keeps track of environment and function names in their model
objects. Getting fully qualified function names dynamically is challenging in R
becausse packages are first class environments that are constructed piecemeal
and all functions are by default anonymous objects that may be bound to a name
in their lexical scope. \emph{instrumentr} handles this by assigning names to
environments on package load events and checking on every write if a function is
being bound to a name. Functions can be nested to arbitrary depths in which case
\emph{instrumentr} links model function objects to reflect the parent-child
relationship.
\end{itemize}
%% lazr

For extracting execution
traces, we developed \emph{lazr}, an R library which sits atop
\emph{instrumentr}. \emph{lazr} collects information about function calls,
arguments, side-effects, and reflective environment access. from the model
objects provided by the following instrumentr event: \emph{function entry and
  exit}, \emph{promise evaluation entry and exit}, \emph{variable reads and
  writes}, \emph{promise value and expression reads}, and \emph{function
  lookup}. This information is stored in compressed tabular format using R's
\emph{fst} library. Analysis scripts provided by \emph{lazr} use this data to
synthesize strictness signatures.

zoo::merge.zoo redefines f with different number of parameters.

\subsection{Applying Strictness Signatures}

To apply a function's strictness signature, we force the corresponding
arguments on every call to that fuction. For this, we developed \emph{strictr},
an R package that adds code to force arguments at the top of a function's
definition. R allows packages to register callbacks which are invoked when a
package is loaded by the program. \emph{strictr} sets up a package load callback
for all the packages for which we generate signatures. When a program loads a
package, \emph{strictr}'s callback is invoked. \emph{strictr} reads the
signatures for functions of the loaded package from a file, and injects code in
the function's definition in accordance with the signature. The injected code
first checks for missingness and evaluates the argument only if it is supplied
by the user or has a default value. The arguments are evaluated in the order
specified by the signature. \emph{strictr} also supports nested functions by
recursively descending into the parent function's definitions until it finds the
inner function binding. Signatures are made available to \emph{strictr} as a
directory containing one file per package. Providing signatures in external
files avoids the need to modify the source code of programs. This also enables
easy experimentation with different signature configurations by providing a
different directory location.

Talk about  \code{is_same_body <<- is_reference}

%%% TODO:
%%% \begin{lstlisting}[language=R]
%%% 
%%% 
%%% 
%%% \end{lstlisting}

\section{Corpus of R Programs}

-- number of R packges
-- number of programs
-- number of loc
-- number of R functions
-- number of parameters
-- number of promises
-- functions per package
-- distribution of calls
-- distribution of arguments

\section{Evaluation}

\subsection{Strictness}

\subsection{Robustness}

\subsection{Performance}

\section{Discussion}



\section{Migration Strategy}
- py2to3 does not do a good job
- hack has a simple migration tool
- scala 2 to 3 has migration tools

- If we have signatures for every test on CRAN.
- 


%%\section*{Acknowledgments}
%% TODO: Thank Flip
\bibliography{bib/jv, bib/aviral}

\end{document}
