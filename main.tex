\documentclass[review,creen,acmsmall]{acmart}
%\settopmatter{printfolios=false,printccs=false,printacmref=false}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}   %% For author/year citations
\usepackage{listings,multirow,wrapfig,xspace,paralist}
\usepackage{xcolor,tikz,graphicx, pifont}
\usetikzlibrary{positioning,automata,fit,shapes.geometric,backgrounds,calc}

\newcommand{\authorcomment}[3]{\xspace\textcolor{#1}{{\bf #2} #3}\xspace}
%%\newcommand{\authorcomment}[3]{}
% For author notes:
\newcommand{\AG}[1]{\authorcomment{orange}{AG}{#1}}
\newcommand{\JV}[1]{\authorcomment{red}{JV}{#1}}
\newcommand{\JJ}[1]{\authorcomment{green}{JJ}{#1}}
\newcommand{\SK}[1]{\authorcomment{yellow}{SK}{#1}}
\newcommand{\OF}[1]{\authorcomment{magenta}{OF}{#1}}

% For meta comments:
\newcommand{\isit}[1]{\authorcomment{cyan}{Check}{#1}}
\newcommand{\todo}[1]{\authorcomment{red}{TODO}{#1}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand{\cmark}{\textcolor{green}{\ding{51}}}
\newcommand{\rmark}{\textcolor{blue}{\ding{108}}}

\newcommand{\always}{\emph{Always}\xspace}
\newcommand{\sometimes}{\emph{Sometimes}\xspace}
\newcommand{\sometimesStar}{\emph{Sometimes*}\xspace}
\newcommand{\never}{\emph{Never}\xspace}
\newcommand{\neverStar}{\emph{Never*}\xspace}
\newcommand{\rdyn}{{\sf R-dyntrace}\xspace}
\newcommand{\instr}{{\sf InstrumentR}\xspace}

\definecolor{LightGray}{RGB}{247, 247, 247}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

%% https://www.davehofmann.de/defining-custom-language-templates-for-latex-listings/
% Define Language
\lstdefinelanguage{smalleR} {
  % list of keywords
  morekeywords={
    for,
    if,
    else,
    function
  },
  sensitive=true, % keywords are not case-sensitive
  morecomment=[l]{\#}, % l is for line comment
  morestring=[b]{"} % defines that strings are enclosed in double quotes
}

\lstset{
  language={smalleR},
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=1mm,
  framexrightmargin=1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray},
  alsoletter={., _, $},
  breaklines=true
}

\newcommand{\code}[1]{\lstinline |#1|\xspace}
\renewcommand{\c}[1]{\lstinline |#1|\xspace}
\newcommand{\strictr}{{\sf StrictR}\xspace}
\newcommand{\lazr}{{\sf LazR}\xspace}
\renewcommand{\Rsh}{{\sf\v R}\xspace}
\newcommand{\Rshstrict}{{\sf\v R-strict}\xspace}

\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\config}[1]{configuration \#{#1}}
\newcommand{\cconfig}[1]{Configuration \#{#1}}

\input{macros.tex}

\include{results}

\setcopyright{rightsretained}
\acmPrice{}
\acmDOI{10.1145/3360579}
\acmYear{2019}
\copyrightyear{2019}
\acmJournal{PACMPL}
\acmVolume{3}
\acmNumber{OOPSLA}
\acmArticle{153}
\acmMonth{10}

\begin{document}
\title{Promises Are Made To Be Broken}
\subtitle{Migrating R to Strict
  Semantics}

\author{Aviral Goel}\affiliation{\institution{Northeastern University}\country{USA}}
\author{Jan Ječmen}\affiliation{\institution{Czech Technical University}\country{Czechia}}
\author{Sebastián Krynski}\affiliation{\institution{Czech Technical University}\country{Czechia}}
\author{Olivier Flückiger}\affiliation{\institution{Northeastern University}\country{USA}}
\author{Jan Vitek}\affiliation{\institution{Czech Technical University and Northeastern University}\country{USA}}
\authorsaddresses{}
\renewcommand{\shortauthors}{Goel, et al.}

\begin{abstract}
  Function calls in the R language do not evaluate their arguments; these are
  passed to the callee as suspended computations and only evaluated if needed.
  After 25 years of experience with the language, there are very few cases where
  programmers leverage delayed evaluation intentionally. Being lazy comes at a
  price in performance and complexity. This paper explores how to evolve the
  semantics of a language towards strictness by default and laziness on demand.
  To provide a migration path, it is necessary to provide tooling for developers
  to migrate libraries without introducing errors. This paper reports on a
  dynamic analysis that infers strictness signatures for functions to capture
  both intentional and accidental laziness. To assess the robustness of the
  inferred signatures we tested them on 2,000 R packages and found that
  inference was wrong in only \robustnesResult of the client programs.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002944.10011123.10010912</concept_id>
<concept_desc>General and reference~Empirical studies</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10010517</concept_id>
<concept_desc>Software and its engineering~Scripting languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011039.10011311</concept_id>
<concept_desc>Software and its engineering~Semantics</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{General and reference~Empirical studies}
\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[500]{Software and its engineering~Scripting languages}
\ccsdesc[300]{Software and its engineering~Semantics}

%\keywords{R language, delayed or lazy evaluation}

\maketitle
\section{Introduction}

The R programming language is widely used in data science. Yet many of users are
unaware that function calls have lazy semantics: arguments are suspended
computations that are evaluated if and when they are needed. \citet{oopsla19b}
provided a thorough observational study of R packages. Most packages are written
without reliance on laziness with the exception of meta-programming. This paper
argues that laziness should be the exception in R. We propose to migrate R
programs to an \emph{eager by default, lazy on demand} semantics. We focus on
the question whether on can switch the semantics of a language without causing
undue breakage in legacy code that is in daily use. Even if programmers do not
avail themselves of laziness, their code may accidentally depends on it.

\paragraph{The case for strictness.} Laziness is error-prone, inconsistent,
and costly! At least when combined with side-effects in a language without type
annotations it is. When a function with multiple evaluation orders is provided
effectful arguments, their effects can be ordered randomly. Functional languages
prevent this by controlling effects from their type system. While R users do not
have types, they often force evaluation at function boundaries. R's laziness is
inconsistent as there are points where evaluation is arbitrarily forced, \eg the
right-hand side of assignments and function returns. Furthermore,
object-oriented multiple dispatch evaluates arguments eagerly to obtain their
class. The costs of laziness comes from having to box each argument in data
structure that must be allocated and freed, and the limits this boxing puts on
compiler optimizations.

\paragraph{The case for laziness.} The success of a programming language
comes down to the strength of its ecosystem. With tens of millions of lines of
library code, any change risks breaking something. Preserving the status quo is
a pragmatic choice to protect an ecosystem. There is also a genuine need for
lazy evaluation: it is the building block of R's meta-programming facilities.
Unevaluated arguments can be coerced back to their source code, that code can be
modified and evaluated in an environment of the programmer's choice.
Meta-programming is used for language extension and to create embedded
domain-specific languages. While one could imagine using macros instead, the
number of libraries that would have to be refactored would be significant.

\paragraph{A pragmatic path forward.} How does on migrate an entire ecosystem?
A viable migration path must abide by four tenets: ({\bf T1}) minimal legacy
code changes, ({\bf T2}) semi-automated migration, ({\bf T3}) testable outcomes,
and ({\bf T4}) measurable benefits. We envision a migration path with the
following characteristics. Changes to legacy code are avoided by non-invasive
extensions to the language's syntax and semantics and limited to library
functions that need a lazy semantics. Strictness signatures are inferred from
the tests associated to every R package. For popular libraries, client code is
used as an oracle to check the correctness of inferred signatures. Performance
and memory footprint are improved by compiler optimizations that leverage the
new semantics. This paper aims to evaluate components of this migration
strategy.

\begin{itemize}[---]
\item {\bf StrictR:} a prototype implementation of R whose semantics are to be
  strict by default; only functions that demand lazy evaluation need to have
  strictness signatures.
\item {\bf LazR:} an infrastructure for inferring strictness signatures by
  dynamic program analysis.
\item {\bf \v R-strict:} an optimizing just-in-time compiler for R with a strict
  by default semantics.
\end{itemize}

We conduct several experiments that aim to answer the question whether such a
semi-automated path is viable. Specifically, it should be the case that change
to the code are small, that inference is accurate, and that there are
performance improvements.

The paper reports on two experiments. First, we obtain the 500 most widely used
packages in the R ecosystem and use their tests to infer strictness signatures.
Since \strictr can load signatures from external files, no changes to code are
required. We then use XXX clients of these 500 packages to check the accuracy of
the inference. Only \robustnesResult of the signatures cause \strictr programs
to exhibit observable differences from the reference semantics. Our second
experiment takes computational tasks and runs them in both strict and lazy
modes, we speedups ranging from \speedupRshStrictMin to \speedupRshStrictMax.

Our conclusions are that the approach is viable as user-visible changes are
small and the inference gives promising results. For community review we intend
to further reduce the error rate by improving the precision of our analysis and
using all available clients to infer signatures rather than only the tests
bundled with a package. The performance numbers should be viewed as a limit
study as the baseline is a compiler that already performs aggressive
optimizations. We have not evaluated the benefits of \strictr on the bytecode
compiler used today. It is conceivable that the relative speedups could be more
impressive than in our native compiler, but in the long run we expect users
to move away from bytecode execution.

\paragraph{Availability.} Our work is in open source,
  experiments are available from:\\

  \begin{center}
    URL/of/artifact/goes/here
\end{center}

\newpage
\section{Background}\label{sec:background}

This section sets the stage by introducing related work and giving a brief
overview of R.

\subsection{Related Work}

There are three clusters of related results: research on adding and removing
laziness in functional languages, research on the R language, and approaches to
language migration.

\paragraph{Call by need}  Functional languages with
a call by need evaluation strategy must contend with memory pressure and
associated performance issues due to allocation of a substantial number of
thunks (suspended computations)~\cite{transformopt,stricteffective,opteval}. The
Glasgow Haskell Compiler performs a strictness analysis pass to identify
arguments that can be evaluated eagerly. While most programs benefit from such a
transformation, due to its conservative nature, this pass misses some
opportunities for optimizations. To recover performance programmers can manually
insert strictness annotations to control evaluation; identifying where to put
them, however, can be challenging. \citet{autobahn} proposed Autobahn, a tool
that automatically infers strictness annotations using a genetic algorithm. The
approach relies on dynamic analysis, as it does not approximate and cannot
guarantee termination on all inputs. As the annotations are based on a
heuristic, developers must manually validate their soundness. The authors report
an average 8.5\% speedup (with a maximum speedup of 89\%). \citet{lazyprof}
solve the complementary problem of suggesting laziness annotations for
call-by-value $\lambda$~calculus using dynamic analysis. They introduce the
notion of laziness potential, a predictor of the benefit obtained by making an
expression lazy. They use this as a guide to insert laziness annotations. They
demonstrate benefits on Racket implementations of Okasaki's purely functional
data structures, monadic parser combinators, and an AI game. Our work is similar
to Autobahn in that we infer annotations dynamically and we do not guarantee
soundness. We depart from Autobahn in that we use dynamic analysis of
execution traces to determine strictness. Furthermore, we must deal with
side-effects and reflective operations which adds extra complexity to our
inference algorithm.

\paragraph{The R language} \citet{oopsla19b} investigated the design and use of
laziness in R. They provide a detailed account of the language's evaluation
strategy with a small-step operational semantics and an empirical evaluation of
laziness in 16,707 packages. Their study shows that most of the R code is written
without reliance on, or awareness of, laziness. Out of 388K functions, 83\%
evaluate all of their arguments in a single order across all calls. The authors
found that programmers sometimes force evaluation of arguments at the beginning
of a function to protect their code from non-deterministic effects. Only a
single instance of a lazy data structure could be found. The main \emph{raison
d'\^etre} for delayed evaluation seems to be meta-programming. In that work, a
function was deemed to be strict if it had a single evaluation order for its
arguments. Our approach is inspired by that work, but we do not require a single
evaluation order for a function, instead, we choose to look at clients and
declare a parameter lazy if some clients call the function with effectful
arguments. \citet{oopsla20b} empirically inferred \emph{type} signatures for
functions by observing the type of arguments and return values. The authors
validated these signatures by inserting type checking code inside functions and
monitoring type failures on client programs. This approach inspired our strictness
inference, however, types are easier to check for soundness than strictness. For
types, one simply inserts code that checks that if an argument is eventually
evaluated, it evaluates to the expected type. For strictness, we have to worry
about the interplay of side-effects and changes to the order of evaluation of
arguments. This makes validation of strictness signatures hard.

\paragraph{Language migration} Changing a language with a large codebase
is challenging. No migration has been more fraught than that of Python. Python 3
was released in 2008 with many changes that broke backward compatibility and no
automated upgrade path. \citet{Agg15} attempted to use statistical machine
learning to convert Python 2 to 3. \citet{Pra20} described a tool for
discovering types in Python programs as a combination of probabilistic type
prediction and search-based refinement. Migration was also studied in the
context of Java libraries~\cite{Xu19}, Android apps~\cite{Orso20} and C++
applications~\cite{OB20}. A more successful experience is the migration from PHP
to Hack at Facebook. The key to success was a close feedback loop between
language changes and their impact on the ecosystem at large. As all Hack users
share an employer and a source code repository, it was possible to test changes
and develop tools targeted at relevant usage patterns.\footnote{Private
  communication with the developers of Hack.} One last relevant thread of work
is the migratory typing of \citet{matthias06} where a gradual type system is
added to a variant of the Scheme programming language to enable gradual
migration from untyped to typed code.


\subsection{Laziness and the R Language}

The R language is widely used in data science. R is a vectorized, dynamic, lazy,
functional and object-oriented programming language~\cite{ecoop12}, designed to
be easy to learn by non-programmers and to enable rapid development of new
statistical methods. The language was created in 1993 by~\citet{R96} as a
successor to an earlier language for statistics named S~\cite{S88}.

\paragraph{Functions}
In R every linguistic construct is desugared to a function call, even control
flow statements, assignments, and bracketing. Furthermore, all functions can be
redefined. This makes R both flexible and challenging to compile. A function
definition can include default expressions for parameters, these can refer to
other parameters. R functions are higher-order. The following snippet declares a
function \code f which takes a variable number of arguments, whose parameters
\code x and \code y, if missing, have default expressions \code y and
\code{3*x}, and which are only evaluated when needed. The function returns a closure.
\begin{lstlisting}
 > f <- function(x=y, ..., y=3*x) { function(z) x+y+z }
\end{lstlisting}\vspace{1mm}
\noindent
The \code f function can be called with a single argument matching \code x, as in
\code{f(3)}, with named arguments, as in \code{f(y=4,x=2)}, with a variable
number of arguments, for example \code{f(1,2,3,4,y=5)}, multiple arguments
captured by \code{...}, or with no arguments at all, \code{f()}, which creates a
cyclic dependency. Some functions are written to behave differently in the
presence of missing arguments. To this end the \c{missing(x)} built-in can be
used to check if parameter \code{x} was provided at the call site or not, even
if it was later substituted by a default value. A vararg parameter, written
\code{...}, accepts an arbitrary number of arguments, including missing
arguments. A vararg can be materialized into a list with \code{list(...)}. Most
frequently varargs are forwarded to a called function. This enables the
function to expose its callee's interface to the callers without listing the
callee's parameters and their default values.

\paragraph{Reflection}
R supports meta-programming. The \code{substitute(e,envir)}
function yields the parse tree of the expression \code{e} after performing
substitutions defined by the bindings in \code{envir}.
\vspace{-1mm}
\begin{lstlisting}
 > substitute(expression(a + b), list(a = 1))
 expression(1 + b)
\end{lstlisting}
\noindent
Extensions written in C are able to freely access and mutate the expression, as
well as the memoized result of promises. R allows programmatic manipulation of
parse trees, which are themselves first-class objects. They are evaluated using
the \code{eval(e,envir)} function. In R the local scope of a closure is a
first-class, mutable map. It is possible to access the local environment, but
also to reflectively extract the environments of all functions currently being
present on the call stack.

\paragraph{Effects} While R strives to be functional, it has many imperative
features such as assignment to local variables \code{<-}, assignment to
variables in an enclosing scope \code{<<-}, and assignment in a programmatically
chosen scope \code{assign()}. R supports non-local returns either through
exceptions or by delayed evaluation of a \code{return} statement. Of course,
there are all sorts of external effects and no monads.

\paragraph{Delayed Evaluation}

The combination of side effects, frequent interaction with C, and absence of
types has pushed R to be more eager than other lazy languages. Strictly
speaking, R is not lazy as it evaluates arguments that it does not need to. Let
us review its design. Arguments to a function are bundled into a thunk called a
\emph{promise}. Logically, a promise combines an expression's code, its
environment, and its value. To access the value of a promise, one must
\emph{force} it. Forcing a promise triggers evaluation and the computed value is
captured for future reference. The following snippet defines a function \code{f}
that takes an argument \code x and returns \code{x+x}. When called with an argument
that has the side effect of printing to the console, the side effect is
performed once as the second access to the promise is cached.
\begin{lstlisting}
 > f <- function(x) x+x
 > f( {print("Hi!");2} )
 "Hi!"
 4
\end{lstlisting}

\noindent
Promises associated to parameters' default values have access to all variables
in scope, including other parameters. Promises cannot be forced recursively,
that is an error. Promises are mostly encapsulated and hidden from user code. R
only provides a small interface for operating on promises:
\begin{itemize}
\item[] {\hskip -1em \bf\small\code{delayedAssign(x,exp,env,aenv)}}: creates a
  promise with body \code{exp} and binds it to variable \code x (where \code x
  is a symbol). Environment \code{env} is used for evaluation and \code{aenv} to
  perform the assignment.
\item[] {\hskip -1em \bf\small\code{substitute(e,env)}}: substitutes variables
  in \code e with their values found in environment \code{env}, returns an
  \code{expression} (a parse tree).
\item[] {\hskip -1em \bf\small\code{force(x)}}: forces the promise \code x. This
  replaces a common programming idiom, \code{x<-x}, which forces \code x by
  assigning it to itself.
\item[] {\hskip -1em \bf\small\code{forceAndCall(n,f,...)}}: calls \code{f} with
  the arguments specified in the varargs, of which the first \code{n} are forced
  before the call.
\end{itemize}

\noindent
While R does not provide built-in lazy data structures, they can be encoded. R
evaluates promises aggressively. The sequencing operator \code{a:b} will
evaluate both \code a and \code b, assignment \code{x<-a} evaluates \code a, and
\code{return} also triggers evaluation. In addition, many core functions are
strict.

\newpage %% Keeps this starting at the top if possible
%% OKAY LET ME REFORMULATE: KEEP THIS AT TOP OR I GET UPSET
\section{StrictR: R with Strictness Signatures}\label{sec:strictr}

We refer to the R language extended with strictness signatures as \strictr, to
differentiate it from the GNU R reference implementation. To achieve the
expected semantics, we have implemented an R package, also named \strictr,
which, when loaded, applies strictness signatures to all loaded code.

\paragraph{Signature files.}
A signature file contains strictness signatures for functions of one or more
packages. The format is straightforward, a sequence of signatures \emph{sig} of
the form
%
\[
\mathit{sig}~~::=~~\mathsf{strict}~\mathsf{`pack}::\mathsf{fun`}~~\langle i, j, k, \dots\rangle
\]
%
Here, {\sf pack} is the name of the package in which the function occurs, and {\sf
  fun} is its name. The sequence of integers specifies which argument positions
are evaluated lazily.

One drawback of external signatures has to do with inner functions. Functions do
not intrinsically have names --- they are first-class objects, late-bound to any
number of names within environments. Only functions exported to a package
namespace have a canonical name, so to speak. We considered various heuristics,
but some packages dynamically set up methods in package load hooks. This dynamism
makes it hard to identify the correct name. Consider this snippet
which conditionally defines and then re-defines \c f.

\begin{lstlisting}
 merge.zoo <- function(all) {
   f <- if (any(all)) function(a, ret.zoo) # { ... }
        else            function(a, ret.zoo) # { ... }
   # ...
   f <- function(i) # { ... }
 }
\end{lstlisting}

\paragraph{Applying signatures.}
To apply signatures in the GNU R reference implementation, \strictr performs
source-to-source rewriting of functions as they are loaded. The rewriting is
simple; each function must \c{force} every argument that was not declared lazy.
Arguments are checked for missingness before forcing; if they are either not
missing or are missing but have a default value, then \c{force} is called.
Signature files are usually stored in a directory containing one file per
package. Providing signatures in external files avoids the need to modify the
source code of programs and also enables easy experimentation with different
signature configurations. The implementation uses a feature of R that allows
registering callbacks when packages are loaded. \strictr registers a callback
that reads the signature file in the current directory or on the load path.
Then, as functions are defined, \strictr injects code in the function in
accordance with the signature. Our first implementation mutated function bodies.
This resulted in failures as the same function object can occur with different
names and different signatures. For instance, in the \code{rlang} package,
\code{is_same_body <<- is_reference} aliases \code{is_reference}. Mutating
\code{is_reference} to make arguments strict inadvertently also makes the
function \code{is_same_body} strict. Further discussion will clarify why this is
undesirable. Suffice it to say that \strictr now copies functions as it rewrites
them.

\paragraph{Extended signature.}
As an alternative to signature files, we implemented an R annotation package
which defines the following annotations: \c{@lazy.file} to say that a
file defaults to lazy, \c{@strict.file} to denote all functions should be
strict, \c{@lazy.param} to denote that an argument must be lazy,
\c{@strict.param} for the converse, and \c{lazy(exp)} to request that an
expression be packed into a promise and evaluated lazily. These annotations deal
with inner functions, but they require source code changes. For the rest of the
paper we assume only external annotations.

\paragraph{Intrinsic laziness.} When should a function argument be lazy?
We have not found significant (barely any) use of functional programming idioms
related to call-by-need, such as infinite data structures. In fact, most code
seems to be written as if R was strict. There is one significant exception to
this: the application of some form of meta-programming to an argument.
Consider the following definition for an example:
\begin{lstlisting}
 f <- function(a,b) { print(deparse(substitute(a)));  x <- eval(substitute(b));  x+a }
\end{lstlisting}

\medskip
\noindent
A call of \c{f(1+2, 3+4)} creates two promises. The first is accessed by
\c{substitute}, turned into a string by \c{deparse} and printed. The code of the
second is accessed by \c{substitute} and evaluated by \c{eval}. Then expression
\c{x+a} forces the first promise; the second is never forced. Both arguments are
intrinsically lazy. Intrinsic laziness is transitive as arguments are passed
from one function to the next. If any C extension accesses a promise through the
\code{PREXPR} macro, the argument has to stay lazy for similar reasons. An
argument may not be evaluated in all calls to the function. This means the
function is not strict in that argument. Unless the documentation dictates that
this argument is evaluated in special cases where it is expected to perform a
reflective or effectful operation, it may still be possible to make the argument
strict at the developer's discretion.

\paragraph{Accidental laziness.} In order to preserve the behavior of legacy
code, some parameters will be labeled as lazy even if the called function does
not require this. An argument that performs a side-effect will be treated as
lazy to retain semantics. For instance in the call \c{f(g(),x<-1)}, function \c
f is free to evaluate its arguments in any order. Enforcing one particular order
may lead to observable differences in behavior, \eg if the call to \c{g()} reads
\c{x}. Writing such code is bad form, as small changes in the implementation of
\c f may break it. R has a call-by-value semantics for vectors and lists. These
are the most frequently used data types, so many updates will be locally
contained. Errors and exceptions are another source of effects inside promises.
Some reflective functions can make evaluation of a promise sensitive to its
position on the call stack, and strict evaluation would be observable:
\code{as.environment} and \code{pos.to.env} use integers to access specific
frames on the stack. It is worth noting, again, that such code is brittle as any
change in the implementation of the target function can change the frame
returned by the reflective calls.

Vararg arguments currently remain lazy. Assigning a single strictness annotation
to $\dots$ is tricky because a function can have different strictness behavior
for the individual elements. This happens in object-oriented dispatch where the
vararg is forwarded from the dispatcher to the target method. One solution would
be to make vararg strict if it is strict in all of its constituents. However,
this may not occur frequently enough to justify the added complexity. The
current choice is pragmatic but imprecise; we are considering options for a more
accurate treatment of varargs.


\paragraph{Order of evaluation} A design choice we faced in our implementation
was to select an evaluation order for strict arguments. One possibility would be
to retain the evaluation order of the function if it is unique, but this
has the disadvantage of being rather obscure to programmers. Another reasonable
choice, which is implemented in our compiler extension, would be to evaluate
strict arguments left-to-right at the call site and then evaluate strict default
values left-to-right in the function prologue. This is likely to be natural to
end-users who write the function call, and thus have control over the order in
which they pass arguments and can reason about their potential side effects. In
the GNU R implementation of \strictr, we picked a third alternative. We evaluate
all arguments left-to-right in the order of definition in the function
signature. This means that end-users need to know in what order arguments are
defined, something that can be awkward as functions can have upward of 20
arguments. The reason for this choice is that \strictr rewrites functions and
not call sites, because at a call site we can't know which function is being
called.\footnote{Shortly before submission, we realized that we could implement
  the same semantics as the compiler since GNU R keeps a \c{PROMARGS} list which
  stores promises in the same order as at the call site.}

\newpage%% Don't mess with me.
\section{LazR: Inferring Strictness Signatures}\label{sec:lazr}

\lazr is an analysis pipeline for inferring strictness annotations in legacy R
libraries at scale. \lazr has two important components; there is a system for
tracing the execution of R scripts, and infrastructure for extracting
executables and running analyses that scale to thousands of packages.

The goal of the analysis performed by \lazr is not to infer maximally strict
signatures for all library functions, but rather to infer signatures that will
minimize the impact of the change on their clients. Thus, we must consider both
intrinsic and accidental laziness. Let us start with Fig.~\ref{iss}. Its
\c{popular} function takes five arguments, one is conditionally evaluated, the
next is forwarded to \c{substitute} for meta-programming, the third is never
used, and the last two are forwarded to a strict function. The client code panel
shows four possible invocations. For \c{r1}, any signature that has at least
variable \c b lazy will return the same result. For \c{r2}, if \c a and \c c are
strict it is possible to observe their evaluation order, as \c{stop()}
terminates execution. For \c{r3}, if \c{d} is strict a different result can be
observed. And finally, for \c{r4}, both \c d and \c{e} must be lazy.

\begin{figure}[!h]
\begin{tabular}{lll}
  \begin{minipage}{7cm}
\begin{lstlisting}
 r1<-popular(1,1+2,3,4,5)
 r2<-popular(c=stop(),a=print(r1),b=1+2,0,9)
 r3<-popular(1,1+2,3,r1<-4,r1+1)
 r4<-popular(1,1+2,3,r1,r1<-0)
\end{lstlisting}
\end{minipage}
&&
\begin{minipage}{5cm}
\begin{lstlisting}
 popular<-function(a,b,c,d,e){
  if (something) a
  print(c<-substitute(b))
  return e+d
 }
\end{lstlisting}
\end{minipage}\\
\it Client code&& \it Library
\end{tabular}%
\caption{Inferring strictness signatures}\label{iss}
\end{figure}

To summarize, the expected output of the analysis of legacy code is to increase
strictness while keeping the number of observable semantic differences low. For
new code, we expect programmers to specify what needs to be lazy in their
documentation and mostly use strict parameters.

\subsection{Tracing the execution of R scripts}

\begin{figure}[h!]
  \centering
  \scalebox{0.85}{
    \begin{tikzpicture}
      \definecolor{maroon}{HTML}{D5A6BD}
      \definecolor{darkmaroon}{HTML}{741B47}
      \definecolor{yellow}{HTML}{FFF2CC}
      \definecolor{darkyellow}{HTML}{D6B656}
      \definecolor{blue}{HTML}{DAE8FC}
      \definecolor{darkblue}{HTML}{6C8EBF}
      \definecolor{green}{HTML}{D5E8D4}
      \definecolor{darkgreen}{HTML}{82B366}
      \definecolor{orange}{HTML}{FFE6CC}
      \definecolor{darkorange}{HTML}{D79B00}
      \definecolor{red}{HTML}{F8CECC}
      \definecolor{darkred}{HTML}{B85450}
      \definecolor{purple}{HTML}{E1D5E7}
      \definecolor{darkpurple}{HTML}{9673A6}
      \definecolor{gray}{HTML}{F5F5F5}

      \newcommand{\nodesep}[0]{0.030 \textwidth}
      \newcommand{\textsep}[0]{0.010 \textwidth}
      \newcommand{\backopacity}[0]{0.9}

      \newcommand{\nodename}[1]{\normalsize \begin{tabular}{c}#1\end{tabular}}
      \newcommand{\nodedesc}[1]{\small \begin{tabular}{c}#1\end{tabular}}

      \tikzstyle{block}     = [rectangle, rounded corners, minimum width=.08 \textwidth, minimum height=35pt]
      \tikzstyle{connector} = [line width=0.25mm, ->]

      \node [block, draw = darkmaroon, very thick, fill = maroon]                                (install)   {\nodename{Install\\CRAN}};
      \node [block, draw = darkyellow, very thick, fill = yellow, right = \nodesep of install  ] (extract)   {\nodename{Extract\\Programs}};
      \node [block, draw = darkblue,   very thick, fill = blue  , right = \nodesep of extract  ] (trace)     {\nodename{Trace}};
      \node [block, draw = darkgreen,  very thick, fill = green , right = \nodesep of trace    ] (reduce)    {\nodename{Reduce}};
      \node [block, draw = darkorange, very thick, fill = orange, right = \nodesep of reduce   ] (combine)   {\nodename{Combine}};
      \node [block, draw = darkred,    very thick, fill = red   , right = \nodesep of combine  ] (summarize) {\nodename{Summarize}};
      \node [block, draw = darkpurple, very thick, fill = purple, right = \nodesep of summarize] (report)    {\nodename{Report}};

      \draw [connector] (install)   edge (extract);
      \draw [connector] (extract)   edge (trace);
      \draw [connector] (trace)     edge (reduce);
      \draw [connector] (reduce)    edge (combine);
      \draw [connector] (combine)   edge (summarize);
      \draw [connector] (summarize) edge (report);

      \node [below = \textsep of install]   (installdesc)   {\nodedesc{8 Hours\\17,398 Packages}};
      \node [below = \textsep of extract]   (extractdesc)   {\nodedesc{5 Minutes\\500 Packages\\24,141 Scripts}};
      \node [below = \textsep of trace]     (tracedesc)     {\nodedesc{24 Hours\\240,520 Files\\966 GB}};
      \node [below = \textsep of reduce]    (reducedesc)    {\nodedesc{6 Hours\\XX Files\\38 TB}};
      \node [below = \textsep of combine]   (combinedesc)   {\nodedesc{1 Hour\\XX Files\\36.5 GB}};
      \node [below = \textsep of summarize] (summarizedesc) {\nodedesc{53 Minutes\\9 Files\\107 MB}};
      \node [below = \textsep of report]    (reportdesc)    {\nodedesc{XXs\\XX Files}};

      \begin{scope}[on background layer]
        \node[fit=(install) (extract), rectangle, rounded corners, fill=gray, fill opacity=\backopacity, label=above:\textbf{Corpus}]          (corpus) {};
        \node[fit=(trace),             rectangle, rounded corners, fill=gray, fill opacity=\backopacity, label=above:\textbf{Tracing}]         (exectrace) {};
        \node[fit=(reduce) (report),   rectangle, rounded corners, fill=gray, fill opacity=\backopacity, label=above:\textbf{Post-processing}] (postprocessing) {};
      \end{scope}
    \end{tikzpicture}
  }
  \caption{Analysis Pipeline}\label{fig:analysisPipeline}
\end{figure}

The heart of \lazr is a dynamic analysis tool built on the \rdyn package which
extends the GNU R virtual machine version 4.0.2~\cite{oopsla19b}.

When \lazr executes an R script, it generates an \emph{execution trace} which is
a sequence of low-level events that closely follow the operations performed by
the GNU R interpreter. As traces can get extremely large, rather than recording
them, the infrastructure exposes callbacks that are used to hook
analysis-specific functionality to low-level events in the interpreter. The
trace exposes raw R objects being operated on as
well as control flow through the code.

To provide a layer of abstraction, \lazr maintains \emph{model objects} to
abstract from the concrete R data structures. The model objects represent,
among others, function instances, the hierarchy of environments, and frames
on the call stack. Model objects are particularly helpful for handling some of the
complexities of R. For instance, model objects have unique identities, whereas R
objects are identified by their memory address, and the garbage collector can
reuse these. \lazr takes care of memory management for model objects and
provides efficient indexing. Model functions have names that are heuristically
reconstructed and keep track of their lexical scoping structure. The model stack
can deal with R's pervasive use of \c{longjump} for non-local returns. Lastly,
\lazr maintains a notion of logical time, used to record when some
events of interest happened. For instance, model environments record the time of
last read and last write. Similarly, functions and promises have the start and
end of their evaluation recorded. Logical time is used, for example, to decide
if an event happened while a promise was executing, or the ordering of events
such as reads and writes.

\subsection{Inferring signatures from an execution trace}

\lazr monitors execution traces, looking for signals that a certain function
parameter must be evaluated lazily. These are the signals that we use:
\begin{itemize}
\item[{\bf M}:] Any parameter that is provided to the \c{substitute} function is
  being \emph{meta-programmed}. This is a strong intrinsic signal that the parameter
  must be lazy.
\item[{\bf U}:] Some parameters may remain \emph{unevaluated}. This is a signal
  of intrinsic laziness. It is weaker than M as we do not know if the contract
  of the function is to be lazy in that parameter.
\item [{\bf S}:] Some arguments may have observable \emph{side-effects}. This is
  a weak signal of accidental laziness as some effects may be benign.
\item[{\bf R}:] Some arguments may use \emph{reflection} to observe the state of
  the call stack. This is also a signal of accidental laziness.
\end{itemize}
\noindent
While we cannot assess intent, our goal is to not break client code, \ie
identify laziness required to preserve the result, regardless of whether it is
accidental or not.

For any given execution trace, \lazr models each argument of each loaded
function. If a function is invoked, the execution trace will record all
operations related to the evaluation of promises and correlate them with
function parameters.

\paragraph{Unevaluated and Meta-programmed parameters (U/M)}
If there is an invocation of some function \c f, for which one of its parameters
\c p remains unevaluated when the entire script terminates, then a U is recorded
for \c{f.p}. It may be the case that some invocations of \c f do evaluate \c p.
The same applies for parameters that flow into the \c{substitute} function; they
are marked as M.

\paragraph{Side-Effects (S)}
There are three categories of effects we are interested in: reads of variables
in environments, writes (definitions, updates, and removals) of variables, and
errors.

For any write, \lazr checks if there is an active promise on the call stack; if
not, the write is ignored. Then it checks if the write is \emph{visible}, a
write is only visible if the environment being updated pre-dates the promise
being evaluated. Writes that are not visible are ephemeral and cannot be
observed. If the write is visible, then the active promise is marked as the
\emph{direct} cause of the effect and the event is recorded for later analysis.
If there are multiple promises being evaluated, only the first of those is the
direct cause; others are \emph{transitive}.

Reads are treated in a similar way. In addition, \lazr also checks if the
environment being read was modified between the logical time the promise that
caused the read was created and before it started evaluating. If the environment
was not modified, evaluating the arguments strictly has no consequence. On the
other hand, if it was updated, then strict evaluation could return a different
value.
For efficiency reasons, \lazr does not record which variables are written
to in an environment. Thus the read effects may have some false positives.

When an error is encountered during execution, either due to an explicit call to
\c{stop} or an error such as accessing an undefined variable, \lazr marks all
active promises as causes.

Once a promise has exhibited an effect in any invocation of a function, the
model parameter corresponding to that promise is marked with an S.

\paragraph{Reflective arguments (R)}
When a promise calls \code{as.environment(v)} or \code{pos.to.env(v)}, and
\c v is \c{-1}, then the model parameter corresponding to that promise should be
marked R. \lazr uses the same approach as above, marking the topmost active
promise as the direct cause, subsequent promises are transitively responsible.

\medskip

Promises that are transitively responsible for an S or R event have their
corresponding parameter marked as S or R, respectively. This is conservative but
slightly imprecise.

\newpage
\subsection{Scaling to collections of scripts}

The final result of analyzing a single R script is a table summarizing, for each
function and each argument, which signals were observed. Multiple scripts can be
straightforwardly merged as the union of signals for each parameter.

The amount of data that has to be analyzed can reach multi-terabyte sizes. \lazr adopts
a simple map-reduce style. Each analysis is split in 4 phases. The reduce maps
a function on the output of one trace to get a partial summary. The combine
phase concatenates partial summaries. Then, the summarize phase aggregates
summaries into a result table. At this point, the file is less than 100MB in
size. Finally, the report phase creates graphs and tables for inclusion in
the paper.

The \lazr pipeline is set up with a container image that includes all the
dependencies for installing analysis code and R packages. This provides a
reliable reproducible setup across machines. To run it, we mirror the CRAN and
Bioconductor repositories and install their R packages. This is followed by the
generation of execution traces. These traces are analyzed to generate tabular
data files and strictness signatures. Our analysis pipeline is managed by a
Makefile that has rules for every step. This makes it easy to administer the
experiments on multiple machines. Whenever possible, we parallelize the steps
using GNU parallel~\cite{gnuparallel}. Our experiments were performed on two Intel
Xeon 6140, 2.30GHz machines with 72 cores and 256GB of RAM each.

\subsection{Selecting a corpus}\label{sec:corpus}

For this paper, a corpus of 500 packages from CRAN~\cite{ligges2017} and
Bioconductor~\cite{bioc} was selected on 26th July 2021, 2 PM UTC, based on
their number of clients, \emph{i.e.}, the packages which are the most widely
used in the ecosystem. The chosen packages have a total of 15,362 clients. The
most widely used package, \c{ggplot2}, has 2,669 clients, and the package with
the fewest clients in our corpus, \c{spatstat.linnet}, has 5. The selected 500
packages have 2.2M lines of R code and 2.7M lines of native code. For
synthesizing signatures for their functions, \lazr extracts all executable
artifacts from each package.

\begin{wraptable}{r}{7cm}
  \vspace{-3mm}
  \small
  \centering
  \caption{Corpus}\label{table:corpus}
  \vspace{-3mm}
  \begin{tabular}{lrrr}
    \toprule
    &\bf Tests&\bf Examples&\bf Vignettes\\
    \midrule
    {Scripts}&5.8K&20.2K&631\\
    \midrule
    {LOC}&406.4K&207.0K&48.7K\\
    \bottomrule
  \end{tabular}

  \medskip\medskip
  \medskip
  \caption{Allocations} \label{table:allocations}
  \centering
\begin{tabular}{ll}
  \begin{tabular}{lr}
    \toprule
    \bf Object&\bf Count\\
    \midrule
    Promise&16.0B\\
    Environment&6.8B\\
    Logical&6.1B\\
    Character&5.5B\\
    Language&2.9B\\
    Integer&2.5B\\
    List&849.7M\\
    Real&639.0M\\
    Closure&585.6M\\
    \bottomrule
  \end{tabular}&
  \begin{tabular}{lr}
    \toprule
    \bf Object&\bf Count\\
    \midrule
    Char&326.4M\\
    Raw&288.7M\\
    Symbol&204.7M\\
    Dot&157.9M\\
    Externalptr&66.6M\\
    S4&30.3M\\
    Complex&2.2M\\
    Expression&1.6M\\
    Weakref&25.2K\\
    \bottomrule
  \end{tabular}\end{tabular}
\end{wraptable}%%

Table~\ref{table:corpus} summarizes the runnable code in the corpus. Each test
file is run as a separate script. Examples and vignettes are snippets of code
embedded in the documentation. \lazr extracts them into self-standing scripts.
Typically, vignettes are longer examples that come with input data, while
examples are smaller code fragments meant to illustrate the use of the package's
functions.

%% \begin{table}
%%   \vspace{-3mm}
%%   \small
%%   \caption{Events} \label{table:events}
%%   \centering
%%   \begin{tabular}{lrr}
%%     \toprule
%%     \bf Event&\bf Count& \bf Time\\
%%     \midrule
%%     Alloc&125.3B&104.9h\\
%%     Dealloc&101.6B&37.5h\\
%%     EvalEntry&91.5B&9.9h\\
%%     EvalExit&90.5B&9.7h\\
%%     VarLkp&89.4B&101h\\
%%     FunLkp&47.0B&13.7h\\
%%     SplEntry&29.1B&23.2h\\
%%     \bottomrule
%%   \end{tabular}
%%   \begin{tabular}{lrr}
%%     \toprule
%%     \bf Event&\bf Count& \bf Time\\
%%     \midrule
%%     SplExit&28.1B&8h\\
%%     VarDef&20.9B&33.4h\\
%%     BtnEntry&14.5B&9.5h\\
%%     BtnExit&14.5B&5.6h\\
%%     PromEntry&13.2B&12.1h\\
%%     PromExit&13.2B&5.8h\\
%%     VarAsn&7.6B&5.6h\\
%%     \bottomrule
%%   \end{tabular}
%%   \begin{tabular}{lrr}
%%     \toprule
%%     \bf Event&\bf Count& \bf Time\\
%%     \midrule
%%     ClosureEntry&6.6B&234.5h\\
%%     ClosureExit&6.3B&3.1h\\
%%     PromLkp&5.8B&1.8h\\
%%     PromExpr&2.8B&1.8h\\
%%     VarRemoval&360.7M&10.4m\\
%%     PromSubst&101.0M&4.7m\\
%%     Error&494.0&0.1s\\
%%     \bottomrule
%%   \end{tabular}
%% \end{table}

During execution, \lazr collects information about function calls, arguments,
side-effects, and reflective environment accesses from the model objects provided.
This information is stored in compressed tabular format using R's \c{fst}
library. To give an idea of the scale of the analysis,
Table~\ref{table:allocations} presents the number of model objects for which
\lazr recorded information.

The packages in the corpus have 50,435 top-level functions. We ignore anonymous
and inner functions. Table~\ref{table:packsize} shows the distribution of these
functions across packages. We observe that 148 packages have 25 functions or
less. As expected, there is a steady decrease in packages with more functions.
There are 12 packages with more than 500 functions. The \c{spatstat.geom} package
has the maximum number of functions, 886.

\begin{wraptable}{r}{8cm}
  \vspace{-3mm}
  \small
  \caption{Package Size} \label{table:packsize}
  \centering
  \begin{tabular}{lr}
    \toprule
    \bf Functions&\bf Packages\\
    \midrule
    1--25&148\\
    26--50&98\\
    51--75&52\\
    76--100&33\\
    101--150&54\\
    151--200&34\\
    201--250&28\\
    \bottomrule
  \end{tabular}
  \quad
  \begin{tabular}{lr}
    \toprule
    \bf Functions&\bf Packages\\
    \midrule
    251--300&16\\
    301--400&14\\
    401--500&7\\
    501--600&5\\
    601--700&1\\
    701--800&4\\
    801--900&2\\
    \bottomrule
  \end{tabular}
\end{wraptable}


We observe 137.2M calls to these functions. Figure ~\ref{fig:callDist} shows the
distribution of calls across functions called in the synthesis
phase. 49.5\% of functions are called more than ten times. 16.4\% of functions are
called only once, leading to low coverage. These functions are spread across 414
packages.
We observe 294.45M arguments created in our execution traces. Of those, 3.9M are
missing, 18.9M are $...$ arguments, and 271.6M are promises. These arguments
correspond to 185.9K parameter positions of the 50,435 functions for which we
generate signatures.
7.3\% of functions have 0 parameters, 19.6\% have 1, and 5.0\% have over 10. There
are 15 functions with over 50 parameters that come from 10 packages. Of those,
\c{rpart.plot::check.if.dot.arg.supported.by.rpart.rules} has the highest number,
122 parameters.
%
\begin{figure}[!h]
  \centering
  \input{graphs/call_dist.tex}
  \caption{Call Distribution}
  \label{fig:callDist}
\end{figure}
%

\subsection{Inferred strictness signatures}\label{sec:results}

\lazr obtained data for 50,435 functions and 186K parameters. The results are
signals (M, U, S, R) for individual parameters. There were 9,100 parameters that
were marked M for meta-programmed. Table~\ref{table:strictdist} summarizes the
distribution of the combination of the other signals; note that functions and
packages can be counted in multiple rows as different parameters fall in
multiple categories. Rows can be interpreted as follows: the first row, for
instance, shows that 128K parameters coming from 44K functions and 489 packages
were not marked in any way. The second row shows that only 134 parameters
were marked R only for their use of reflective operations.

The data yields the following insights. First, the majority of parameters, 65\%,
are always evaluated, and the corresponding arguments do not perform
side-effects or reflective operations. They can become strict. Of the remaining,
there are 13\% of unevaluated parameters, the most significant potential source of
laziness. This is followed by 4\% of parameters whose arguments are marked S
only. The rest of the configurations are quite low. Another observation is that
rows marked R come from very few functions and packages.

\begin{table}
  \vspace{-3mm}
  \small
  \caption{Signature Summary} \label{table:strictdist}
  \centering
  \begin{tabular}{ccccccrrrrr}
    \toprule
    \bf ... & \bf M & \bf ? & \bf U & \bf S & \bf R & \multicolumn{2}{c}{\textbf{Parameters}} & \multicolumn{2}{c}{\textbf{Functions}}& \bf Packages\\
    \midrule
    \xmark{}&\xmark{}&\xmark{}&\xmark{}&\xmark{}&\xmark{}&148.4K&72.8\%&49.3K&93.4\%&489\\
    \xmark{}&\xmark{}&\xmark{}&\xmark{}&\xmark{}&\cmark{}&529&0.3\%&509&1\%&119\\
    \xmark{}&\xmark{}&\xmark{}&\xmark{}&\cmark{}&\xmark{}&1.3K&0.7\%&950&1.8\%&207\\
    \xmark{}&\xmark{}&\xmark{}&\xmark{}&\cmark{}&\cmark{}&76&0\%&74&0.1\%&22\\
    \xmark{}&\xmark{}&\xmark{}&\cmark{}&\xmark{}&\xmark{}&28.5K&14\%&12.4K&23.6\%&450\\
    \xmark{}&\xmark{}&\xmark{}&\cmark{}&\xmark{}&\cmark{}&33&0\%&32&0.1\%&24\\
    \xmark{}&\xmark{}&\xmark{}&\cmark{}&\cmark{}&\xmark{}&314&0.1\%&226&0.4\%&98\\
    \xmark{}&\xmark{}&\xmark{}&\cmark{}&\cmark{}&\cmark{}&9&0\%&9&0\%&5\\
    \rmark{}&\rmark{}&\cmark{}&\rmark{}&\rmark{}&\rmark{}&3.2K&1.6\%&1.7K&3.2\%&240\\
    \rmark{}&\cmark{}&\rmark{}&\rmark{}&\rmark{}&\rmark{}&1.3K&0.6\%&825&1.6\%&118\\
    \cmark{}&\rmark{}&\rmark{}&\rmark{}&\rmark{}&\rmark{}&20.0K&9.8\%&20.0K&37.9\%&430\\
    \bottomrule
  \end{tabular}
\end{table}


The inferred strictness signatures for library functions use these marks to
decide which arguments should be lazy and which should be strict. While any
parameter marked M \emph{must} be lazy, there is some freedom to decide for the
other combinations. Our evaluation will probe the different configurations and
show the error rates they may have.

\medskip

We now discuss the different sources of laziness in more detail.


\subsubsection{Metaprogramming}

\begin{wraptable}{r}{5.5cm}
  \small
  \centering
  \caption{Metaprogramming}\label{table:metaprogramming}
  \vspace{-3mm}
  \begin{tabular}{lrrr}
    \toprule
    &\bf R&\bf Native&\bf Total\\
    \midrule
    {Arguments}&\MetaCountArgumentsR&\MetaCountArgumentsNative&\MetaCountArgumentsTotal\\
    {Parameters}&\MetaCountParametersR&\MetaCountParametersNative&\MetaCountParametersTotal\\
    {Functions}&\MetaCountFunctionsR&\MetaCountFunctionsNative&\MetaCountFunctionsTotal\\
    {Packages}&\MetaCountPackagesR&\MetaCountPackagesNative&\MetaCountPackagesTotal\\
    \bottomrule
  \end{tabular}
\end{wraptable}


Meta-programming tracks calls to \code{substitute} (from R) and \code{PREXPR}
(from native code). Table~\ref{table:metaprogramming} shows the number of
arguments, parameters, functions, and packages using metaprogramming. Out of
\AG{total number of arguments} model arguments, \MetaCountArgumentsTotal were
meta-programmed. This results in \MetaCountParametersTotal lazy parameters from
\MetaCountFunctionsTotal functions of \MetaCountPackagesTotal packages. While
more arguments are metaprogrammed using \code{substitute} than \code{PREXPR},
they correspond to fewer parameters.

For a discussion of the use of \code{substitute} we refer the reader to
\citet{oopsla19b}. We elaborate on \code{PREXPR} since it was not addressed
there. This macro is used by many packages to extract a promise's expression for
ad-hoc evaluation strategies. We found its uses in packages like \code{lazyeval}
and \code{rlang}. The \code{rlang} package particularly stands out because it
provides its own API for meta-programming. The \code{PREXPR} macro is also used
by the builtin functions of the GNU R interpreter, a canonical example is the
\code{missing} function to check if an argument was provided. Unlike user
packages, these uses of \code{PREXPR} don't require the corresponding arguments
to be lazily evaluated. Hence, parameters metaprogrammed using \code{PREXPR}
from GNU R interpreter are not made lazy. They are also not included in
Table~\ref{table:metaprogramming}.


\subsubsection{Missing Arguments}

\begin{wraptable}{r}{6cm}
  \small
  \centering
  \caption{Missing}\label{table:missing}
  \vspace{-3mm}
  \begin{tabular}{lrrr}
    \toprule
    &\bf Sometimes&\bf Always&\bf Total\\
    \midrule
    {Argments}&\MissingSometimesCountArguments&\MissingAlwaysCountArguments&\MissingTotalCountArguments\\
    {Parameters}&\MissingSometimesCountParameters&\MissingAlwaysCountParameters&\MissingTotalCountParameters\\
    {Functions}&\MissingSometimesCountFunctions&\MissingAlwaysCountFunctions&\MissingTotalCountFunctions\\
    {Packages}&\MissingSometimesCountPackages&\MissingAlwaysCountPackages&\MissingTotalCountPackages\\
    \bottomrule
  \end{tabular}
\end{wraptable}

Of the 294M total arguments in our corpus, \MissingTotalCountArguments arguments
were missing. This excludes the \c{...} and metaprogrammed parameters which
could also be missing, but are separately discussed above. These
\MissingTotalCountArguments arguments correspond to \MissingTotalCountParameters
parameters. Arguments to \MissingAlwaysCountParameters of these parameters are
always missing. These parameters are classified as \always, in contrast to the
\MissingSometimesCountParameters \sometimes parameters which are sometimes
passed an argument. The distribution of these two categories is presented in
Table~\ref{table:missing}. The \always parameters are treated as lazy, whereas
the \sometimes parameters are treated as lazy if they don't evaluate the
arguments passed to them at least once. These parameters are dealt like other
parameters in Section~\ref{subsubsection:unevaluted_arguments} discussed below.

The example below shows an example of \always missing parameter. The \c{names}
argument was missing in all the calls to this function defined in the \c{rlang}
package. Since the function does not use this argument, it is present presumably
for future evolution or backwards compatibility.

\begin{lstlisting}
new_environments <- function(envs, names) {
  stopifnot(is_list(envs))
  structure(envs,
            names = map_chr(unname(envs), env_name),
            class = "rlang_envs")
}
\end{lstlisting}

The example below shows an example of \sometimes missing parameter. The
\c{linewidth} argument was missing in some calls to this function defined in the
\c{base64enc} package. The function assigns a default value of \c{0L} to
\c{linewidth} if it is missing.

\begin{lstlisting}
base64encode <- function(what, linewidth, newline) {
  linewidth <- if (missing(linewidth) || !is.numeric(linewidth) ||
                  length(linewidth) < 1L) 0L
               else as.integer(linewidth[1L])
    ...
}

\end{lstlisting}


\subsubsection{Unevaluated Arguments} \label{subsubsection:unevaluted_arguments}

\begin{wraptable}{r}{6cm}
  \small
  \centering
  \caption{Unevaluated}\label{table:unevaluated}
  \vspace{-3mm}
  \begin{tabular}{lrrr}
    \toprule
    &\bf Sometimes&\bf Never&\bf Total\\
    \midrule
    {Parameters}&\UnevaluatedSometimesCountParameters&\UnevaluatedNeverCountParameters&\UnevaluatedTotalCountParameters\\
    {Functions}&\UnevaluatedSometimesCountFunctions&\UnevaluatedNeverCountFunctions&\UnevaluatedTotalCountFunctions\\
    {Packages}&\UnevaluatedSometimesCountPackages&\UnevaluatedNeverCountPackages&\UnevaluatedTotalCountPackages\\
    \bottomrule
  \end{tabular}
\end{wraptable}

Of the 294M total arguments in our corpus, \UnevaluatedTotalCountArguments of
promises were not evaluated. They correspond to \UnevaluatedTotalCountParameters
parameters from \UnevaluatedTotalCountFunctions functions of
\UnevaluatedTotalCountPackages packages. We can classify these parameters into
two categories: parameters that are \sometimes evaluated and parameters that are
\never evaluated. Table~\ref{table:unevaluated} presents the number of
parameters, functions, and packages in these two categories. There are
\UnevaluatedSometimesCountParameters \sometimes parameters and
\UnevaluatedNeverCountParameters \never parameters. These may correspond to code
paths not exercised or to dummy parameters.
%
A common pattern is the following:
%
\begin{lstlisting}
 lazyeval::%||% <- function(x, y) if(is.null(x)) y else x
\end{lstlisting}
%
This function evaluates \code{y} only if \code{x} is \code{NULL}, making
\code{y} a \sometimes parameter.
\noindent
Another pattern is to delay the evaluation of a parameter.

\begin{lstlisting}
 glue::on_package_load <- function(pkg, expr) {
   if (isNamespaceLoaded(pkg)) { expr }
   else {
     thunk <- function(...) expr
     setHook(packageEvent(pkg, "onLoad"), thunk)
   }
 }
\end{lstlisting}
%
Here, \code{expr} may be delayed until \code{pkg} is loaded. Thus, \code{expr}
should not be evaluated strictly.
%
S3 generics result in many \sometimes and \never parameters. These functions
dynamically dispatch to a specific implementation based on the argument's class.
In the following example, the first argument is always evaluated, but \code{n}
and \code{m} are only evaluated sometimes and \code{r} never.
%
\begin{lstlisting}
 abind::acorn <- function(x, n=6, m=5, r=1, ...) UseMethod('acorn')
\end{lstlisting}
\noindent
One source of \never parameters is when a set of functions implements a common
interface, but not all functions need all parameters. The \code{proxy} package
defines over a dozen methods with the interface \code{function(a,b,c,d,n)} that
compute different proximity metrics using a subset of the arguments.

Arguments can be \never by design; \code{tail} is not defined for
\code{tbl_lazy} objects and throws an error when called.
\begin{lstlisting}
 dbplyr::tail.tbl_lazy <- function(x, n = 6L, ...)
   stop("tail() is not supported by sql sources", call.=FALSE)
\end{lstlisting}
%
\never parameters can also represent an erroneous condition that would not
happen in a correct program. The following function fails if its argument does
not have the correct type. In all observed calls, \code{signal}
went  unused because \code{e} had the right type.
\begin{lstlisting}
 codetools::checkSymOrString <- function(e, signal = stop) {
   type <- typeof(e)
   if (type == "symbol" || type == "character") e
   else signal("not a symbol or string")
 }
\end{lstlisting}

\subsubsection{Side-Effects}

Of all the promises observed by \lazr, only 0.5\% perform side-effects. Many
side-effects are benign for our purposes because they happen in environments
that are scoped by the evaluation of the promise, and the side-effects are ignored.

\begin{table}[!h]  \vspace{-3mm}  \small
  \caption{Effects} \label{table:effects} \centering
  \begin{tabular}{llllll}    \toprule
    \textbf{Effect}&\textbf{Count}&\textbf{Arguments}&\textbf{Parameters}&\textbf{Functions}&\textbf{Packages}\\    \midrule
    L&3.7M&1.2M&5.6K&3.7K&359\\
    D&1.1M&159.0K&3.0K&2.6K&319\\
    A&235.8K&122.7K&511.0&487.0&109\\
    R&2.8K&718.0&27.0&24.0&10\\    \bottomrule
  \end{tabular}
\end{table}

\noindent
Table~\ref{table:effects} counts lookups (L), definitions (D), assignments (A),
and removals (R), and the arguments, parameters, functions, and packages directly
responsible for those effects. Lookups are the most common cause for making
arguments lazy. This is followed by definitions and assignments. Removing
bindings from environments is not a common operation.
%
Comparing the number of events in this table against the total number of events,
we narrow 89B variable lookups down to 3.7M, 20B definitions to 1.1M, and 7B
assignments to 235K.

\begin{wraptable}{r}{5cm}
  \vspace{-3mm}
  \small
  \caption{Effect Sequence} \label{table:effectseq}
  \centering
  \begin{tabular}{lll}
    \toprule
    \textbf{Sequence}&\textbf{Arguments}\\
    \midrule
    -&99.5\%\\
    L+&0.41\%\\
    D+&0.04\%\\
    A+&0.03\%\\
    (L+D+)+&0.01\%\\
    \bottomrule
  \end{tabular}
\end{wraptable}

An argument can perform a combination of effects during evaluation.
Table~\ref{table:effectseq} shows the sequence of operations performed by the
arguments in our corpus. The majority of arguments, 99.5\% to be precise, do not
perform any relevant effect. 0.41\% only perform a sequence of lookups, 0.04\%
perform a sequence of definitions, 0.03\% perform assignments, and 0.01\%
perform a sequence of lookups and definitions. From this, we conclude that
the majority of arguments have a simple pattern of events.

A common source of non-local writes is the \c{shiny} package used for building
interactive web applications. Consider the call to \code{bindEvent} which
attaches an observer expression to an event. The observer performs a non-local
side-effect using \code{<<-}, this is a common way to update the global state.
%
\begin{lstlisting}
 bindEvent(trigger(), x=observe({vals <<- c(vals, val())}))
\end{lstlisting}
%
\noindent
Another source of non-local writes occurs in the \code{withr} package. It
provides functions to evaluate code in a temporarily modified global state. These
code blocks assign their result in the current scope. In the following example, a
side-effecting expression is passed to \code{with\_seed} which changes the
random number generation seed.
%
\begin{lstlisting}
 with_seed(seed <- sample.int(.Machine$integer.max, 1), runif(5))
\end{lstlisting}
%
\noindent
Many side-effects occur in test cases written with the \code{testthat} library.
The global state is modified temporarily to test its effect on the function
under consideration.
%
Another common pattern is when the result of an intermediate computation is
assigned to a variable for subsequent use. In the call below, the intermediate
result is assigned to \code{x} before being passed on to the
\code{as.data.table} function.
%
\begin{lstlisting}
 as.data.table(x <- as.character(sample(letters, 5)))
\end{lstlisting}
%
\noindent
Deleting a variable from an environment is done with the \code{rm} function.
The following function computes the number of rows of a data frame and assigns
it to a variable \code{.N} in the caller's environment. Eventually, it removes the
variable.
%
\begin{lstlisting}
 [.data.table <- function (x, i, j, ...) {
   assign(".N", nrow(x), envir=parent.frame(), inherits=FALSE)
   if (remove.N) rm(list=".N", envir=parent.frame())
   ...
\end{lstlisting}
\noindent
Lastly, \code{rlang} defines a function \code{env_bind} which can be used to
create and remove bindings from an environment. When passed with the value
\code{zap()}, it removes the corresponding symbol from the environment. It uses
the C function \code{R_removeVarFromFrame}.

\subsubsection{Reflection}
Reflective code is brittle. The following example shows that code that looks up
its call stack is sensitive to small implementation changes, such as the
addition of a call to an identity function. The evaluation of \c x and \c y will
yield different results as the latter is executing within the \c{id} function.
%
\begin{lstlisting}
 id <- function(a) a
 f <- function(x, y) { x; id(y); }
 f(as.environment(-1), as.environment(-1))
\end{lstlisting}
%
\noindent
This can transitively affect the strictness of other parameters. If \code{f} is
called from \code{g} as shown below, we will have to make \code{g} non-strict in
\code{u} and \code{v}. Their results depend upon how they are evaluated inside
\code{f}.

\begin{lstlisting}
 g <- function(u, v) { f(u, v) }
 g(as.environment(-1), as.environment(-1))
\end{lstlisting}
%
\noindent
R provides other functions for reflective stack access, \code{parent.frame} and
\code{sys.frame}; these are less brittle as they access frames relative to the
promise's creation environment.

\begin{wraptable}{r}{5.5cm}
  \small
  \centering
  \caption{Reflection}\label{table:reflection}
  \vspace{-3mm}
  \begin{tabular}{lrrr}
    \toprule
    &\bf Direct&\bf Transitive&\bf Total\\
    \midrule
    {Arguments}&\RefCountArgumentsDirect&\RefCountArgumentsTransitive&\RefCountArgumentsTotal\\
    {Parameters}&\RefCountParametersDirect&\RefCountParametersTransitive&\RefCountParametersTotal\\
    {Functions}&\RefCountFunctionsDirect&\RefCountFunctionsTransitive&\RefCountFunctionsTotal\\
    {Packages}&\RefCountPackagesDirect&\RefCountPackagesTransitive&\RefCountPackagesTotal\\
    \bottomrule
  \end{tabular}
\end{wraptable}

Table~\ref{table:reflection} presents the number of arguments that call
\code{as.environment(-1)} and \code{pos.to.env(-1)} directly or transitively.
This excludes the cases where the argument is also metaprogrammed. Two
parameters from two functions, \code{R.oo::.getFunctionByName}, and
\code{backports:::get0} call these functions directly.
%
The first searches for a function by name in different scopes. Its argument has
a default value of \code{as.environment(-1)}.
%
\begin{lstlisting}
 R.oo:::.getFunctionByName <- function(..., callEnvir = as.environment(-1L), ...) {
   envirT <- callEnvir
   ...
\end{lstlisting}
%

\section{Robustness of inferred signatures} \label{Evaluation:Robustness}

\begin{figure}[th!]
  \centering
  \scalebox{0.85}{
    \begin{tikzpicture}

      \definecolor{gray}{HTML}{F5F5F5}
      \definecolor{green}{HTML}{D5E8D4}
      \definecolor{darkgreen}{HTML}{82B366}
      \definecolor{yellow}{HTML}{FFF2CC}
      \definecolor{darkyellow}{HTML}{D6B656}
      \definecolor{blue}{HTML}{DAE8FC}
      \definecolor{darkblue}{HTML}{6C8EBF}
      \definecolor{purple}{HTML}{E1D5E7}
      \definecolor{darkpurple}{HTML}{9673A6}
      \definecolor{red}{HTML}{F8CECC}
      \definecolor{darkred}{HTML}{B85450}

      \tikzstyle{block}      = [rectangle, rounded corners, minimum width=.12 \textwidth, minimum height=35pt]
      \tikzstyle{dummyblock} = [rectangle, rounded corners, minimum width=.10 \textwidth, minimum height=22pt]

      \tikzstyle{connector} = [line width=0.25mm, ->]

      \newcommand{\nodesep}[0]{0.060 \textwidth}
      \newcommand{\dummynodesep}[0]{8mm}
      \newcommand{\textsep}[0]{10mm}
      \newcommand{\backopacity}[0]{0.9}

      \newcommand{\nodename}[1]{\large \begin{tabular}{c}#1\end{tabular}}
      \newcommand{\nodedesc}[1]{\small \begin{tabular}{c}#1\end{tabular}}

      \node [block,      draw = darkgreen,  very thick, fill = green]                                                           (corpus)            {\nodename{Corpus}};
      \node [block,      draw = gray,       very thick, fill = gray,   right = \nodesep of corpus]                              (dummyrun)          {};
      \node [block,      draw = darkyellow, very thick, fill = yellow, above = \dummynodesep of dummyrun.north, anchor = north] (runone)            {\nodename{First\\Run}};
      \node [block,      draw = darkyellow, very thick, fill = yellow, below = \dummynodesep of dummyrun.south, anchor = south] (runtwo)            {\nodename{Second\\Run}};
      \node [block,      draw = darkblue,   very thick, fill = blue,   right = \nodesep of dummyrun]                            (outone)            {\nodename{Compare\\Output}};
      \node [block,      draw = darkpurple, very thick, fill = purple, right = \nodesep of outone]                              (strictrun)         {\nodename{Run with\\StrictR}};
      \node [dummyblock, draw = gray,       very thick, fill = gray,   above = 0mm of strictrun.north]                          (dummystrictrun)    {};
      \node [dummyblock, draw = gray,       very thick, fill = gray,   below = 0mm of strictrun.south]                          (dummystrictruntwo) {};
      \node [block,      draw = darkred,    very thick, fill = red,    right = \nodesep of strictrun]                           (outtwo)            {\nodename{Compare\\Output}};

      \draw [connector] (corpus)       -- (runone.west);
      \draw [connector] (corpus)       -- (runtwo.west);
      \draw [connector] (runone.east)  -- (outone);
      \draw [connector] (runtwo.east)  -- (outone);
      \draw [connector] (outone)       -- (strictrun);
      \draw [connector] (strictrun)    -- (outtwo);
      \draw [connector] (runtwo.south) -- ++(0, -0.2) -| ($(runtwo) !1.0! (outtwo.south)$);

      \node [below = \textsep of corpus]    (corpusdesc)     {\nodedesc{XXs\\XX Packages\\XX Files}};
      \node [below = \textsep of dummyrun]  (rundesc)        {\nodedesc{XXs}};
      \node [below = \textsep of outone]    (outonedesc)     {\nodedesc{XXs\\XX Files}};
      \node [below = \textsep of strictrun] (strictrundesc)  {\nodedesc{XXs\\XX Signatures}};
      \node [below = \textsep of outtwo]    (outtwodesc)     {\nodedesc{XXs\\XX Files}};

      \begin{scope}[on background layer]
        \node[fit=(corpus) (runone) (runtwo) (outone),                       rectangle, rounded corners, fill=gray, fill opacity=\backopacity, label=above:\textbf{Deterministic Corpus}] (detcorpus) {};
        \node[fit=(strictrun) (dummystrictrun) (dummystrictruntwo) (outtwo), rectangle, rounded corners, fill=gray, fill opacity=\backopacity, label=above:\textbf{Failures}] (failures) {};
      \end{scope}

    \end{tikzpicture}
  }
  \caption{Validation Pipeline}\label{fig:validationPipeline}
\end{figure}

In this section, we evaluate the robustness of strictness signatures generated
by \lazr. This is done by running scripts extracted from 2,000 client packages of
our corpus. These packages have 4.5M lines of R code and 4.7M lines of native
code. Table ~\ref{table:clientcorpus} gives the number of scripts and lines of code
exercised.

\begin{wraptable}{r}{6cm}  \vspace{-3mm}  \small  \centering
  \caption{Client Corpus}\label{table:clientcorpus}  \vspace{-3mm}
  \begin{tabular}{lrrr}    \toprule
    &\bf Tests&\bf Examples&\bf Vignettes\\
    \midrule
    {Scripts} &9.8K&41.1K&1.7K\\
    \midrule
    {LOC} &751.2K&348.8K&112.3K\\    \bottomrule
  \end{tabular}
\end{wraptable}%%

Using \lazr, we extracted 52K scripts from the clients. From these, we filtered
scripts whose output was non-deterministic, leaving us with 42K scripts. The
goal of the next step is to validate if, after applying strictness signatures,
they produce identical outputs.

Comparing the output of scripts is standard practice in the R community for
detecting regressions. There may be a difference in execution that does not
manifest in the output; this may hide some errors, so the results reported here
should be considered a lower bound.

\begin{wraptable}{r}{5cm}
  \small
  \caption{Strictness Failure} \label{table:strictfail}
  \centering
  \begin{tabular}{lc|ll}
    \toprule
    \#&\textbf{Configuration}&\multicolumn{2}{c}{\textbf{Failures}}\\
    \midrule
    0&$+U+S+R$&239&0.56\%\\
    1&$+U+S-R$&234&0.55\%\\
    2&$+U-S+R$&1922&4.51\%\\
    3&$+U-S-R$&6625&15.54\%\\
    4&$-U+S+R$&610&1.43\%\\
    5&$-U+S-R$&616&1.44\%\\
    6&$-U-S+R$&2319&5.44\%\\
    7&$-U-S-R$&7158&16.79\%\\
    \bottomrule
  \end{tabular}
\end{wraptable}

Table~\ref{table:strictfail} reports on the eight configurations we evaluated.
For each row in the table, we ran all 42K scripts and counted the number of
differences from expected outputs. Each row corresponds to a different set of
strictness signatures for the corpus. The 9K parameters that were inferred to be
meta-programmed (M) are always lazy. The 128K parameters that were not marked
were always strict. For the 36K remaining parameters (those with a combination
of U, S, R), each row represents a different strictness setting.

For each configuration, we report the number and percentage of failing programs.
The \emph{+} sign indicates the corresponding parameter is lazy and \emph{-}
means strict. For example, in the $+U+S+R$ configuration, any parameter marked
as either U, S or R, is lazy, whereas the configuration $+U+S-R$ means that any
parameter marked U or S must be lazy, and the configuration $-U+S+R$ means that
parameters marked S and R must be lazy.

\cconfig 0 is the laziest of the configurations; only 239 scripts out of 42K
have erroneous outputs. These errors come from some of the
unmarked parameters. Consider, for instance, a library function \c f that was
always called without side-effects during inference and for which there is a
side-effecting client. \lazr would mark that function as strict, but the client
may be able to observe the semantic difference. Luckily, this is quite rare.

All other configurations are strictly stricter, and thus are likely to experience
more errors. The case of \config 1 is surprising, as making arguments reflective
hides 5 errors. This is an outlier that should be elided.

In the extreme, \config 7 makes all parameters marked either U, S, or R strict.
This leads to 16\% of failures which is rather high. This suggests a substantial
amount of accidental laziness which can be observed.

One takeaway from this evaluation is that unevaluated parameters (U) have only a
small impact on correctness, while S and R are the sources of most errors.

From the results, we conclude that using \config 0 as the starting point for
migration is the safest. The first attempt at removing laziness at scale can rely on
this signature configuration. Next, reflective and unevaluated arguments can be
made strict without a significant increase in failure rate. Side-effecting
arguments result in many failures.

\section{\Rsh: Compiling the Lazy Away}\label{sec:rsh}

In this section we report on our experiments to evaluate the implications of
eagerness on an implementation of R. To that end we modified \Rsh, a just-in-time
compiler for the language. \Rsh is based on the GNU R reference implementation
and introduces an additional two-tiered compilation strategy. The
first tier is realized by a naive bytecode interpreter, the second tier
by an optimizing native compiler. The compiler employs, among many other
optimizations, speculative inlining of R closures and promises \citep{dls19,
oopsla20c}.
The reason for choosing \Rsh as the target for our
experiments is that it allows us to better evaluate the impact of laziness. In
\Rsh we can both measure the impact on an interpreter with few optimizations
applied to the bytecode, and an optimizing compiler that already does its best
to elide promises (as long as the default semantics allows it).

We conducted limit-study experiment to assess the highest achievable performance
by removing maximum possible promises over an aggresively optimizing compiler.
We do this by changing the evaluation semantics of the \Rsh VM. This is an isolated
experiment and does not involve neither \strictr nor \lazr in any ways.

For our evaluation, we changed the first-tier
compiler to eagerly evaluate all arguments at all call-sites, except for a
manually curated list of exceptions in order to preserve appropriate semantics.
This eager bytecode also serves as the source code for the optimizing tier. In the following
text we refer to our implementation as \Rshstrict. Note that a realistic implementation
would, instead, re-write externally provided function annotations and rely on already
existing speculative monomorphization of call-sites in \Rsh.

Our main hypothesis regarding performance is that eager semantics lead to faster
execution of R programs. This hypothesis might seem unexpected to some readers,
as call-by-need is sometimes used to avoid unnecessary computation in a program.
On the other hand, delaying computations is more complex to implement and we
observed it to obstruct performance in two ways. First, it leads to more
allocations of promises used to represent the delayed computation. Second, in
combination with effects potentially originating from promises, it obstructs
more advanced compiler analyses and optimizations. Thus, we expect the hypothesis
to also hold for a just-in-time compiler that uses advanced optimizations.
In particular, we pose the following predictions:
(1) eager semantics should improve performance of most benchmarks, for both tiers
of \Rsh; (2) a significant portion of the speedup is due to reduced garbage
collection pressure; (3) and there is an additional speedup due to improved compiler
optimizations. In the following, we will present our evidence in support of (1)
and (3), and partial evidence for (2).

\paragraph{Methodology}

We evaluate the effects of our modifications on the \Rsh benchmark suite.
To ease the discussion, the results are presented on a representative subset of
the suite which includes one variant of
each benchmark. As should be no surprise given the results presented so far,
only a handful of functions, such as
\lstinline{tryCatch}, had to be kept lazy for all the benchmarks to compute
the correct results.
All of the following experiments are run on a dedicated benchmarking machine, with
all background tasks disabled. The system features an Intel i7-6700K CPU, stepping 3,
microcode 0xe2 with 4 cores and 8 threads. The system has 32 GB of RAM and is
running Ubuntu 18.04 on a 4.15.0-136 Linux kernel. For ease of use, experiments
are built as containers, based on Ubuntu 20.04, and executed on
the Docker runtime 20.10.5, build 55c4c88. We verified the overhead introduced by
the containerization to be uniform. All measurements are recorded repeatedly and we
keep a historical record to spot unstable behavior. This led us to exclude the
\lstinline{convolution} benchmark from the suite, which appears to have a
bi-modal performance profile, likely caused by the LLVM backend of \Rsh.

Performance measurements are gathered by running $t_e$ invocations of \Rsh on
each benchmark. Within each invocation we measure the execution time of $t_i$
in-process invocations. For each invocation, the first 5 in-process iterations
are discarded to exclude warmup behavior. Aggregate numbers are reported as the
speedup over the arithmetic mean of the execution times. Multiple speedup
numbers are averaged using a geometric mean. To establish a baseline we measured
the speedup of \Rsh over GNU R, on our subsetted benchmark suite with parameters
$t_e = 1, t_i = 15$. We found a mean speedup of \speedupRsh, ranging between
\speedupRshMin and \speedupRshMax.

\paragraph{Speedup}

\begin{figure}[h]
  \centering
  \input{graphs/rshStrictPerf.tex}
  \caption{Speedup of \Rshstrict}
  \label{fig:speedup}
\end{figure}

First, we compare \Rsh against \Rshstrict. This experiment estimates the end-to-end
improvement on performance that a change to eager semantics in the R language
would have on \Rsh. The execution times were measured with $t_e = 4, t_i = 25$.
\autoref{fig:speedup} shows a boxplot for the speedups of \Rshstrict along the X-axis,
normalized with respect to \Rsh (lazy). Outliers are represented by black dots. Overall, we observe a mean speedup of
\speedupRshStrict, ranging from \speedupRshStrictMin to \speedupRshStrictMax.
For \speedupRshStrictSignificant out of \benchmarkSuiteSize benchmarks we measure a significant increase in performance.
%
\begin{figure}[h]
  \centering
  \input{graphs/rshStrictPerfBc.tex}
  \caption{Speedup of \Rshstrict without optimizations}
  \label{fig:speedup-bc}
\end{figure}
%

Then, we repeated the performance experiment, but with the second tier
optimizer completely disabled. In other words, we compare non optimized variants of \Rshstrict vs. \Rsh.
Since this variant executes overall \rshBCSlowdown slower, we chose to only run with $t_e = 1, t_i = 15$.
The results are presented in \autoref{fig:speedup-bc}. We found that the bytecode
interpreter also gains a speedup of \speedupBCRshStrict, ranging from
\speedupBCRshStrictMin to \speedupBCRshStrictMax.
%
Thus, we conclude that in our benchmark suite both a naive interpreter and a speculatively
optimizing native compiler achieve better performance on a strict dialect of R.
Even though the speedup is very similar in numbers, the reasons seem to be
different at times. We will get back to that point in the discussion at the end of
this section.

\paragraph{Garbage Collection}

\begin{figure}[h]
  \centering
  \input{graphs/rshPromNorm.tex}
  \caption{Promises allocated, normalized to GNU R. The right column shows GNU R's actual promises. If the bar is too small to display, we show the actual number of promises in the respective experiment.}
  \label{fig:gc-pressure}
\end{figure}

We measure the number of promises allocated in the
benchmark suite per iteration for GNU R, \Rsh, and \Rsh-strict. The results are shown in
\autoref{fig:gc-pressure}. We report on the geometric mean of the reductions in each benchmark.

The first jump from GNU R's bytecode interpreter to the optimizing just-in-time compiler \Rsh,
leads to a \promiseAlocationReductionGnurRsh reduction in allocations. The number is reached by taking
the geometric mean of light-gray bars, yielding a \promiseAllocationGnurRsh. Therefore, the reduction
is \promiseAlocationReductionGnurRsh ({$100\%$\xspace} - \promiseAllocationGnurRsh).
Note that for some benchmarks the light-gray bar is too small to be seen and replaced by the actual number.

The second step from lazy to eager semantics leads to \promiseAlocationReductionRshStrictToZero
benchmarks not allocating any promises at all. On the remainder, we see an additional
\promiseAlocationReductionRshStrict reduction (\Rshstrict normalized to \Rsh).

Overall, \promiseAlocationReductionRshStrictToZero benchmarks reduce to zero
allocations, the rest reduce on average by \promiseAlocationReductionGnurRshStrict. To reach this number,
once again we compute the geometric mean of dark bars, yielding \promiseAllocatioGnurRshStrict. Therefore, the reduction is ~\promiseAlocationReductionGnurRshStrict ({$100\%$\xspace} -  \promiseAllocatioGnurRshStrict).

The lowest reduction observed is \promiseAlocationReductionGnurRshStrictMin. Surprisingly the number of remaining promises is
still relatively high in some cases. As far as we were able to observe, they
originate largely from special forms, \ie R builtins with a custom evaluation
strategy, that are not yet natively supported in the \Rsh bytecode.

It turns out that even an optimizing compiler has to allocate many promises for R
code, as oftentimes, they cannot be eliminated entirely. \Rshstrict allows for
a larger reduction in the number of promises allocated.
Thus, we expected a significant portion of the overall speedup to originate from
the reduced allocation rate. We measured the differences in garbage collection
time and it ranged from \speedupGCRshStrictMin to \speedupGCRshStrictMax, but
found the contribution to the overall speedup to be smaller than expected.
The GNU R garbage collector, which is reused in \Rsh, has a fairly slow allocation path, which
includes mutating a doubly-linked list. Therefore, some portion of the speedup
could be due to the saved time in the allocation of promises, which is not counted in the GC time.
We therefore conducted an additional experiment, where we evaluated arguments to
calls eagerly, but additionally allocated a promise only to be subsequently discarded.
This configuration led to an overall speedup of
\speedupBCRshStrictAlloc instead of \speedupBCRshStrict, suggesting that
about \speedupDueToReducedGC of the performance improvement in the bytecode interpreter is due to the
reduced allocation. Unfortunately, a similar experiment cannot be as easily conducted
for the optimizing tier of \Rsh, since its compiler would just optimize away all
unnecessary allocations. We conjecture that the contribution should be even
smaller in that case, because it allocates much fewer promises to start with.

\paragraph{Discussion}

In both tiers, we observed a proportional reduction of time spent tracing the
heap and allocating promises. Surprisingly, that was not the main reason
for the speedups. We investigated the remaining difference using the
\lstinline{perf} profiler and found that the overheads of lazy evaluation are to
be found in setting up the execution context for promise evaluation. This
includes marking it as 'executing' to detect recursive evaluation dependencies,
and either calling the interpreter's main eval-loop on the code of the promise or
calling the compiled function. In the case of the native backend, having the
promises inlined at the call-site instead of in a separate native function
invoked by the callee, resulted in fewer instruction cache misses. We also found
some instances where both tiers sped up similarly, however, the underlying
reasons were very different. Take for instance the \lstinline{bin} benchmark,
which showed in both interpreter and compiler a speedup of about $1.5\times$ in
the strict mode. In native, the execution time decreased from 79ms to 53ms. In
the interpreter, time went from 143ms to 97ms per iteration. In the former case,
the speedup comes from the effects described above; for the latter, part of the
speedup is due to better optimizations. Previously the local environment of
the innermost function was live. Thanks to eager evaluation of the arguments,
the argument promises do not leak the environment and therefore \Rsh is able to
elide it.

\section{Conclusion}\label{sec:conclusion}

In R, function arguments are not evaluated at the call-site, instead, the
evaluation is suspended until the callee needs it. The definition of
\emph{need} is quite liberal here as, for example, local re-binding, returning,
and many builtin functions are strict. As was reported in previous work, this
leads to many programs being on the eager side of the spectrum for a lazy
language. Why is R lazy at all? It turns out that allowing users to reflectively
alter argument expressions, before evaluating them, is a very expressive and
powerful meta-programming technique, enjoyed by many package authors in the R
ecosystem to build, \eg embedded domain specific languages. It is part of
what makes R appealing to its users, even if they do not realize that the
language they use has a lazy core. However, the joy is limited when it comes to
writing robust R code --- as both the caller and the callee co-determine what a function
actually does --- and also when implementing the language itself. When we turned
R into a mostly strict mode, the \Rsh just-in-time compiler ran
\speedupRshStrict faster through our benchmarks without any further changes.
Taking everything into account, we believe that R should be strict by default,
giving package authors the option to opt-in to laziness. We provide a strategy
for R as an ecosystem to get there by automatically inferring robust strictness
signatures for package code. These signatures try to capture the desired and
accidental laziness of arguments passed to R packages, thereby allowing most of
the client code to run unchanged --- in our experiments, only \robustnesResult of
all depending packages' tests failed. Such automatically generated strictness
signatures can then subsequently be refined by the package authors and users.
The change to the language would be beneficial in several ways. Implementations
would become faster, compilers and program analysis easier to perform, users
would be presented with a more commonly expected call semantic, and it would open
up the path for further evolution. Currently, many standard techniques such as
gradually typed function signatures and efficient just-in-time optimizations are
difficult to apply to R because of laziness.

\bibliography{bib/jv,bib/aviral,bib/ml,bib/bib}

\appendix

\section{Benchmark Selection}

Table~\ref{table:bms} presents our benchmark selection. We use all the benchmarks
from the \emph{language shootout} suite. We have acquired one real-world program,
\c{flx}, and we have ported three programs from the \emph{Are we fast} suite to
R. All of these are available with our artifact.

\begin{table}[!h]
  \vspace{-3mm}
  \small
  \caption{Benchmarks}\label{table:bms}
  \vspace{-3mm}
  \begin{tabular}{lll}
    \toprule
    \bf Id&\bf Benchmark&\bf Suite\\
    \midrule
    bnc&Bounce&Are we fast\\
    mnd&Mandelbrot&Are we fast\\
    sto&Storage&Are we fast\\
    flx&Flexclust&Real thing\\
    bin&Binarytrees&Shootout\\
    fst&Fasta&Shootout\\
    far&Fastaredux&Shootout\\
    fnk&Fannkuchredux&Shootout\\
    knu&Knucleotide&Shootout\\
    nbo&Nbody&Shootout\\
    pdg&Pidigits&Shootout\\
    rgx&Regexdna&Shootout\\
    rev&Reversecomplement&Shootout\\
    spn&Spectralnorm&Shootout\\
    \bottomrule
  \end{tabular}
\end{table}


\end{document}
